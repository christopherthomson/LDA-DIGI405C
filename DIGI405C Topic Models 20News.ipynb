{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2ff85-b251-4dd1-9c69-40af07976cc0",
   "metadata": {},
   "source": [
    "# DIGI405 - Topic Models with tomotopy \n",
    "With this notebook you can load the 20 Newsgroups dataset and create topic models using the Python package tomotopy. Tomotopy is a Python extension of *tomoto* (*to*pic *mo*deling *to*ol), which is a [Gibbs-sampling](https://www.youtube.com/watch?v=BaM1uiCpj_E) based topic model library written in C++. \n",
    "\n",
    "Read more in the tomotopy documentation... \n",
    "## <img src=https://bab2min.github.io/tomotopy/tomoto.png align=\"left\" width=\"20\">[**tomotopy**](https://bab2min.github.io/tomotopy/v0.12.2/en/)\n",
    "\n",
    "Work through the notebook. There are some tasks and questions to reflect on as you go.\n",
    "\n",
    "You might think about how the output of this topic model might be used. You will see that some topics cover documents from multiple newsgroups - how could this be useful? How does a distribution of topics differ from having a single label?\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Note:</strong> Throughout the notebook there are questions to consider or defined tasks for you to do. Watch out for them - they will have a box around them like this! To get the most out of the activity we recommend you complete these and make some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-machinery",
   "metadata": {},
   "source": [
    "## Import the necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860bc2b-9ac7-495a-b9a4-e9ede8de88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various packages\n",
    "from zipfile import ZipFile\n",
    "import os.path\n",
    "from os import path\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "# from collections import Counter, Iterable\n",
    "import datetime\n",
    "\n",
    "# topic modeling / nlp packages\n",
    "import tomotopy as tp\n",
    "from tomotopy.utils import Corpus\n",
    "\n",
    "# visualisation / exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tmplot\n",
    "from IPython.display import IFrame, Markdown, display\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# These last lines of code suppress deprecation warnings displaying in the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-madrid",
   "metadata": {},
   "source": [
    "### Install Python packages if needed\n",
    "\n",
    "If you get an import error it's likely you need to install tomotopy or tmplot. In that case, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac217a1-8cc8-424b-8434-41829f859ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tomotopy tmplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d2e8-4f3f-4b58-a329-96a861fd5b0e",
   "metadata": {},
   "source": [
    "Run to download NLTK stopwords and functions for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32bb18-a0a3-4098-898b-dcab478603f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-deficit",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "\n",
    "The following cell contains a function to preprocess the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c66c2-df0b-43bc-9f61-129eb0ec31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set, extra_stopwords={}):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    \n",
    "    # replace all newlines or multiple sequences of spaces with a standard space\n",
    "    doc_set = [re.sub(r'\\s+', ' ', doc) for doc in doc_set]\n",
    "    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # add any extra stopwords\n",
    "    if (len(extra_stopwords) > 0):\n",
    "        en_stop = en_stop.union(extra_stopwords)\n",
    "    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        filtered_tokens = [token for token in tokens if token.isalpha() and len(token) > 2]\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [token for token in filtered_tokens if not token in en_stop]\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc95a-b5a4-4aa9-8df4-16fbe91317af",
   "metadata": {},
   "source": [
    "## Load and pre-process the corpus\n",
    "\n",
    "Load the 20 Newsgroups corpus and preprocess it. After tokenising, removing non-alphanumeric tokens, 2-letter words and NLTK stopwords, we also remove short documents that contribute fewer than 5 words to our 'bag of words'. We also (hopefully) remove duplicate posts, and set aside a small sample that we will use for inferring topics for unseen documents later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a704f8e-82b2-415e-bb97-80166574fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "df.columns = ['text', 'target']\n",
    "df['bow'] = doc_clean = preprocess_data(pd.Series(df['text']))\n",
    "targets = pd.DataFrame(newsgroups_train.target_names)\n",
    "targets.columns=['newsgroup_title']\n",
    "df = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "\n",
    "# clean up the dataset a little\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df = df[df['bow'].map(len) > 5] # remove empty or short documents\n",
    "sample = df.sample(n=5, random_state=31) # extract 5 sample documents\n",
    "df = df[~(df.index.isin(sample.index))]\n",
    "df.reset_index(drop=True, inplace=True) # remove gaps in our index after tidying up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8c1b2-941f-41d8-92a6-30c154cca9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2a1fe-4e05-491f-8538-98753931c668",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "In the cell below you can set the parameters of the LDA topic model. Leave them as default the first time you train the model. More information about the tomotopy LDA model parameters can be found [here](https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel).\n",
    "\n",
    "* α – alpha, a Dirichlet prior on the per-document topic distribution\n",
    "* β – beta / eta, a Dirichlet prior on the per-topic word distribution\n",
    "* optim_interval - how often to optimise the beta hyperparameter\n",
    "* k – the number of topics in the model\n",
    "* burn-in – the number of burn-in iterations\n",
    "* iter – the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum frequency of words (integer)\n",
    "# Words with a smaller document frequency than min_df are excluded from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "# For more info see https://bab2min.github.io/tomotopy/v0.12.3/en/#vocabulary-controlling-using-cf-and-df\n",
    "min_doc_freq = 5\n",
    "\n",
    "# Number of top words to be removed (integer)\n",
    "# Setting this to 1 or more removes common words from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "remove_top = 0\n",
    "\n",
    "# Number of topics to return, between 1 and 32767\n",
    "num_topics = 20\n",
    "\n",
    "# You can read more about the following alpha and beta hyperparameters here:\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "# Alpha\n",
    "# Hyperparameter of Dirichlet distribution for document-topic\n",
    "# Controls the density of topics per document\n",
    "# a float\n",
    "doc_topic = 0.1\n",
    "\n",
    "# Beta\n",
    "# Hyperparameter of Dirichlet distribution for topic-word\n",
    "# Note this is 'eta' in tomotopy - it's not a typo!\n",
    "# Controls the density of words per topic\n",
    "# a float\n",
    "topic_word = 0.01\n",
    "\n",
    "# Set the burn-in\n",
    "# Number of initial iterations that are discarded before optimising hyperparameters\n",
    "# This speeds up the convergence of the model on an optimal set of topics\n",
    "brn_in = 10\n",
    "\n",
    "# Number of iterations of the Gibbs sampler\n",
    "# If we specify 30 here, we will run 300 (10*30) iterations of Gibbs sampling total in the training loop\n",
    "num_iterations = 30\n",
    "\n",
    "# Set the top n words from the topic to display in the output of results\n",
    "num_topic_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-forwarding",
   "metadata": {},
   "source": [
    "## Train the model and display the results\n",
    "Run the cell below to train the model and display the results (you shouldn't need to change anything here). **Important:** Each time you change settings of the LDA model, you will need to re-run the cell below to re-train the model. Because LDA is unsupervised and probabalistic, it will produce different results each time. We can control this using a random seed (the `seed` parameter below), but it is worth remembering that these models are very sensitive to changes in their input and we should think of them as approximations of some latent topics, and there is really no single 'correct' model.\n",
    "\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Look through the output. Consider the impact of the different parameter settings on the results. There are several ways to optimise and tweak the results. Try to:\n",
    "    \n",
    "- test more or fewer topics.\n",
    "- test more or fewer number of iterations\n",
    "- test the `min_doc_freq` and `remove_top` variables\n",
    "    \n",
    "Which parameter settings produce the clearest topic groupings?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3d45c",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d97b-63e2-4011-a51f-80e086333740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://bab2min.github.io/tomotopy/v0.12.2/en/\n",
    "\n",
    "# Intialize the model\n",
    "\n",
    "# The default term weighting (\"ONE\") is used below - all terms are weighted equally\n",
    "# \"PMI\" - Pointwise mutual information - or \"IDF\" - inverse document frequency - can also be used for term weighting\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE,\n",
    "                    min_df=min_doc_freq, \n",
    "                    rm_top=remove_top, \n",
    "                    k=num_topics, \n",
    "                    alpha=doc_topic, \n",
    "                    eta=topic_word,\n",
    "                    seed=77,\n",
    "                   )\n",
    "\n",
    "model.burn_in = brn_in\n",
    "\n",
    "# Add each document to the model\n",
    "for text in df['bow']:\n",
    "    model.add_doc(text)\n",
    "\n",
    "print(\"Topic Model Training...\\n\")\n",
    "\n",
    "# train the model\n",
    "# the loop reports LL/word every 10 iterations\n",
    "# this is a measure of model fit to the data (higher is better)\n",
    "for i in range(0, 10):\n",
    "    model.train(iter=num_iterations)\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print(f'\\nTop 10 words of topic #{topic_number}\\n')\n",
    "    print(model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print()\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7beb7b-202b-41d7-8194-14008c51d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Summary\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the frequent words removed by rm_top. \n",
    "# A useful filter, esp with \"ONE\" term weighting\n",
    "model.removed_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b1684-fa7e-4765-8287-df2f616e4946",
   "metadata": {},
   "source": [
    "## Visualise the topic model\n",
    "\n",
    "We are using the tmplot visualisation library, which is strongly influenced by pyLDAvis and the R library LDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6e963-b0cd-4c20-b944-c9b3cd991828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a truncated list of docs for display in tmplot\n",
    "trunc_docs = [doc[:400] for doc in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4aa5e-2e73-4b87-b4a9-db75aebd5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we supply tmplot's list of docs\n",
    "# tmplot sometimes \n",
    "tmplot.report(model, docs=trunc_docs, width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-incentive",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Take a few minutes to explore the interactive visualisation to explore the topics and see whether the top documents for each topic make sense. Are there good / bad topics here? Make some notes. You can also adjust the &lambda;(Lambda) value if you wish. This is another metric that allows you explore words in topics along a scale between being weighted entirely by the probability of the word given the topic (if &lambda; = 1), to weighted entirely by the marginal term probability (ie relative frequency) of the word in the corpus (if &lambda; = 0). What value of &lambda; filters the topics best?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2afbf4",
   "metadata": {},
   "source": [
    "_Your answer here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bf53d-176a-462b-a756-b1db0217b782",
   "metadata": {},
   "source": [
    "## Examine top documents for a given topic\n",
    "The following code to display the top documents is adapated from [***Topic Modeling - With Tomotopy***](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html) from the book *Introduction to Cultural Analytics & Python* by Melanie Walsh (2021). Because of the messiness of the 20 Newsgroups dataset, some documents may not display correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca5faf-d7bd-487e-a716-fe76434bfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "topic_indiv_150_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words_150 = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=150))\n",
    "    topic_indiv_150_words.append(topic_words_150.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974316dc-6ffb-401c-be8c-3a3cf0e94302",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def make_md(string):\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, sources, topic_indiv_150_words, topic_distributions, topic_index, n):\n",
    "\n",
    "    sorted_data = sorted([(_distribution[topic_index], _document, _sources) \n",
    "                          for _distribution, _document, _sources\n",
    "                          in zip(topic_distributions, docs, sources)], reverse=True)\n",
    "\n",
    "    \n",
    "    top_25 = \", \".join(topic_indiv_150_words[topic_index][:25])\n",
    "    make_md\\\n",
    "    (f\"### Topic {topic_index}\\n\\\n",
    "    \\n{top_25} ...\\\n",
    "    \\n\\n**Note**: the highest ranking 150 words in the topic are shown in bold in each text\\n\\n---\")\n",
    "    \n",
    "    for proportion, doc, source in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_indiv_150_words[topic_index]:\n",
    "            #doc = doc.lower()\n",
    "            if word in doc:\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc)\n",
    "                #doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        \n",
    "        make_md(f'  \\n**Topic Proportion**: {proportion}  \\n**Source**: {source}  \\n**Document**: {doc}  \\n\\n---')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04baae-6fc2-4f76-8e29-dbab4ac786c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the topic number to explore\n",
    "topic_no = 9\n",
    "\n",
    "# Set the number of 'top docs' to display\n",
    "# Some newsgroups posts are long!\n",
    "num_top_docs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7273b-32ac-4de4-b886-4ff4005ff29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top documents for the selected topic, with topic words highlighted\n",
    "\n",
    "get_top_docs(df['text'], \n",
    "             df['newsgroup_title'],\n",
    "             topic_indiv_150_words, \n",
    "             topic_distributions, \n",
    "             topic_index = topic_no, \n",
    "             n = num_top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aa79a",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Choose a topic and note some of its characteristics below: what is the topic about? Do documents from multiple newsgroups appear in it? Are there words in the topic that clearly fit or do not fit together?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212a16a",
   "metadata": {},
   "source": [
    "_Your description of a topic here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84a6eb-ab59-422f-aee2-3d7ac5143cf9",
   "metadata": {},
   "source": [
    "## Infer topics for an unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af10bc-f3ea-4911-a89f-f29466a8ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cbfbe-11bf-42c0-b71e-2ef4c6506b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the document for which you want to infer the topics\n",
    "# Change the row index to any of the five options in the sample dataframe above\n",
    "new_text = sample.at[4186, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85688299-ecb6-4de5-836a-c1aadf1d7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_clean = preprocess_data([new_text], {'applause', 'laughter'})\n",
    "new_doc = model.make_doc(new_text_clean[0])\n",
    "\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6f944-92f5-42e6-b86b-8bfb7d9d3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer topics in the new doc\n",
    "\n",
    "new_doc_topics = model.infer(new_doc)\n",
    "\n",
    "new_doc_list = []\n",
    "\n",
    "for topic, prop in enumerate(new_doc_topics[0].tolist()):\n",
    "    new_doc_list.append((topic,prop))\n",
    "    \n",
    "new_doc_topics_sorted = sorted(new_doc_list, key=lambda x: x[1], reverse=True) # sorts document topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267906b-acce-4388-8bc9-c69a8d86cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display topic proportions inferred for our new document, and words from those topics in descending order\n",
    "\n",
    "for topic, prop in new_doc_topics_sorted:\n",
    "    topic_words = [word for word, prop in model.get_topic_words(topic_id=topic, top_n=num_topic_words)]\n",
    "    print(\"%.3f\" % prop, topic, topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9938393-ebba-45d3-ad43-6bb9dfd6708e",
   "metadata": {},
   "source": [
    "## Find similar documents\n",
    "\n",
    "We can documents with a similar topic distribution using Jensen-Shannon distance. A lower distance score indicates documents have more similar topics.\n",
    "\n",
    "For more information, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05230493-1023-4a6b-90e8-afc58a4c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "jenshan_scores = []\n",
    "\n",
    "# here topic_distributions from the 'Examine top documents' section are used\n",
    "for doc, distribution in enumerate(topic_distributions):\n",
    "    a = new_doc_topics[0].tolist()\n",
    "    b = distribution\n",
    "    jenshan_score = distance.jensenshannon(a, b)\n",
    "    jenshan_scores.append((doc,jenshan_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798d225-24e2-4966-9f0f-212dac5624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_index = sorted(jenshan_scores, key=lambda x: x[1]) # The smaller the Jensen-Shannon distance, the more similar the docs\n",
    "\n",
    "print('Similar documents by Jensen-Shannon distance (lower is better):\\n')\n",
    "# Note the first match will be the sample doc itself!'\n",
    "\n",
    "for i in range(5): \n",
    "    document_id, js_score = dist_index[i]\n",
    "    print('---------',document_id, js_score,'---------')\n",
    "    print(df.at[document_id, 'text'])\n",
    "    print()\n",
    "\n",
    "    # doc_list = []\n",
    "    # for i, text in enumerate(df):\n",
    "    #     doc_list.append((i, text))\n",
    "    \n",
    "    document_topics = enumerate(topic_distributions[document_id]) \n",
    "    document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) # sorts document topics\n",
    "    \n",
    "    # print topic proportions in the matched document\n",
    "\n",
    "    for topic, prop in document_topics:\n",
    "        topic_words = [word for word, prop in model.get_topic_words(topic_id=topic, top_n=num_topic_words)]\n",
    "        if prop > 0.05: # here we set a threshold for which topics to print\n",
    "            print (\"%.3f\" % prop, topic, topic_words)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1827966-9c12-48bb-9aaf-18012ac6c76f",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> How good are the inferred topics for our held-out sample documents? Are some better than others? What if you lower the threshold to 0.01 in the line 'if prop > 0.05:' - do the additional topics look relevant?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca250d6b-0061-4b14-9742-4231e76132b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
