{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2ff85-b251-4dd1-9c69-40af07976cc0",
   "metadata": {},
   "source": [
    "# DIGI405C - LDA Topic Models with Tomotopy\n",
    "## Tiny corpus examples\n",
    "\n",
    "This notebook demonstrates LDA topic modeling with very small numbers of documents and topics. Note: typically you would use this method with at least hundreds of documents, and it works better with longer texts of several hundred words, less well with single sentence documents or social media text.\n",
    "\n",
    "**Important:** Each time you change settings, you will need to re-run the cells that create the topic modelling pipeline.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-seeking",
   "metadata": {},
   "source": [
    "### Imports and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860bc2b-9ac7-495a-b9a4-e9ede8de88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import tomotopy as tp\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import IFrame, Markdown, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac217a1-8cc8-424b-8434-41829f859ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to install tomotopy or seaborn uncomment the relevant lines below\n",
    "# run this cell\n",
    "# re-run the cell above\n",
    "\n",
    "# !pip install tomotopy\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32bb18-a0a3-4098-898b-dcab478603f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note if you get an error with stopwords below then uncomment the following lines and re-run this cell \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943d0c0-6a7e-4edf-992d-9c50005f107b",
   "metadata": {},
   "source": [
    "The following cell contain a preprocessing function will use for all our topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set, extra_stopwords={}):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    \n",
    "    # replace all newlines or multiple sequences of spaces with a standard space\n",
    "    doc_set = [re.sub(r'\\s+', ' ', doc) for doc in doc_set]\n",
    "    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # add any extra stopwords\n",
    "    if (len(extra_stopwords) > 0):\n",
    "        en_stop = en_stop.union(extra_stopwords)\n",
    "    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc95a-b5a4-4aa9-8df4-16fbe91317af",
   "metadata": {},
   "source": [
    "## Create a small example corpus\n",
    "In this section we have three different 'tiny' corpora to experiment with. \n",
    "\n",
    "Start by running the cell for **Example 1**, then skip the *Example 2* and *Example 3* cells and go to the **Train a topic model** section. \n",
    "\n",
    "Work through the rest of the notebook before coming back to try *Example 2* and *Example 3*.\n",
    "\n",
    "### Example 1\n",
    "Here is a corpus with two documents and two very clearly different topics: fruit and pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78647a9-749d-45ee-8984-c3f6d2b6fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_number = '1'\n",
    "\n",
    "document_list = [\"apple orange kiwifruit banana pineapple guava lemon plum\",\n",
    "                 \"dog cat rabbit mouse bird fish goat seamonkey\",   \n",
    "                ]\n",
    "\n",
    "source_list = [\"fruit\", \"pets\"] # this just gives us labels for our documents\n",
    "\n",
    "# You can add extra stopwords here, between the curly brackets, in addition to NLTK's stopword list\n",
    "doc_clean = preprocess_data(document_list, {})\n",
    "\n",
    "docs_and_source = list(zip(source_list, document_list)) # combine source and document lists for later use\n",
    "docs_and_source = [list(item) for item in docs_and_source] # make'em lists for easier reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-direction",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "<strong>Task 1</strong>: Train a model with 2 topics and display 10 words per topic, using this corpus. The code block to train a model is below.\n",
    "<ul>\n",
    "    <li>Each topic is a distribution over all words. What do you notice about the top 10 words for each topic?</li>\n",
    "    <li>What happens if you specify 3 topics - more than the number of documents?</li>\n",
    "</ul>\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-dispatch",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Next is another small corpus with four documents. This time there are two \"obvious\" topics (cats and dogs), but potential to identify more specific topics within these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_number = '2'\n",
    "\n",
    "document_list = [\"The cat (Felis catus) is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae and is often referred to as the domestic cat to distinguish it from the wild members of the family.\",\n",
    "                 \"The dog or domestic dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf. The dog is derived from an ancient, extinct wolf, and the modern wolf is the dog's nearest living relative.\",\n",
    "                 \"Cats are beautiful creatures who have a great deal to offer their human companions. It is thought that cats were first domesticated by humans around 5000 years ago and were used to help protect farmers crops from mice and other rodents.\",\n",
    "                 \"The connection between humans and dogs has been long acknowledged as one of the strongest bonds around - even if, at some point, they will inevitably try to steal a sausage off your plate. Here's why we think dogs are the best pets ever.\"\n",
    "                ] \n",
    "\n",
    "source_list = ['Cat wiki page', 'Dog wiki page', 'Cat-world.com', 'GoodHousekeeping.com' ]\n",
    "\n",
    "doc_clean = preprocess_data(document_list, {})\n",
    "\n",
    "docs_and_source = list(zip(source_list, document_list)) # combine source and document lists for later use\n",
    "docs_and_source = [list(item) for item in docs_and_source] # make'em lists for easier reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-compact",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "<strong>Task 2</strong>: Train topic models with 3, 4 and then 5 topics each. The code block to train the model is below.\n",
    "<p>Note: You'll need to change the number of topics in the line `model = tp.LDAModel(k=2)` in the 'Train a Topic model' cell below. You'll also need to re-run the subsequent cells (noted in italics).</p>\n",
    "<ul>\n",
    "    <li>With a partner, compare the results each of you get for each number of topics. Try to identify groups of words that indicate coherence of topic, eg \"descendant, ancient, extinct\" might indicate an 'origins of dog species' topic. Also look for words that do not so strongly cohere with the topic, or that seem spread across topics.</li>\n",
    "    <li>As you try different numbers of topics, take note of how the words representing are assigned, and consider how well the topic distribution represents each document.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6042e",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57bb3b6",
   "metadata": {},
   "source": [
    "We add more documents here - two from each source, eight in total. We'll use this later to compare the distribution of topics in documents and words in topics betwen Example 2 & 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ac56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_number = '3'\n",
    "\n",
    "document_list = [\"The cat (Felis catus) is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae and is often referred to as the domestic cat to distinguish it from the wild members of the family.\",\n",
    "                 \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth, and retractable claws adapted to killing small prey like mice and rats.\",\n",
    "                 \"The dog or domestic dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf. The dog is derived from an ancient, extinct wolf, and the modern wolf is the dog's nearest living relative.\",\n",
    "                 \"Compared to the dog's wolf-like ancestors, selective breeding since domestication has seen the dog's skeleton greatly enhanced in size for larger types as mastiffs and miniaturised for smaller types such as terriers; dwarfism has been selectively utilised for some types where short legs are advantageous such as dachshunds and corgis.\",\n",
    "                 \"Cats are beautiful creatures who have a great deal to offer their human companions. It is thought that cats were first domesticated by humans around 5000 years ago and were used to help protect farmers crops from mice and other rodents.\",\n",
    "                 \"Every cat owner has seen this; your cat is hungry, he weaves in and out of your legs, you return home from work, he rubs his cheek against your shin, headbutts you, rubs against a sofa, or a corner.\",\n",
    "                 \"The connection between humans and dogs has been long acknowledged as one of the strongest bonds around - even if, at some point, they will inevitably try to steal a sausage off your plate. Here's why we think dogs are the best pets ever.\",\n",
    "                 \"When searching for the best family dog, you might think that a smaller pup is the way to go. But actually, some of the strongest dog breeds, that are known for their ability to serve as guard dogs, also make for great companions.\"\n",
    "                ] \n",
    "\n",
    "source_list = ['Cat wiki page', 'Cat wiki page', 'Dog wiki page', 'Dog wiki page', 'Cat-world.com', 'Cat-world.com', 'GoodHousekeeping.com', 'GoodHousekeeping.com']\n",
    "\n",
    "doc_clean = preprocess_data(document_list, {})\n",
    "\n",
    "docs_and_source = list(zip(source_list, document_list)) # combine source and document lists for later use\n",
    "docs_and_source = [list(item) for item in docs_and_source] # make'em lists for easier reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2a1fe-4e05-491f-8538-98753931c668",
   "metadata": {},
   "source": [
    "## Train a topic model\n",
    "\n",
    "You'll need to re-run the following cells below when you move between examples 1 - 3 above.\n",
    "\n",
    "### Set parameters\n",
    "In the cell below you can set some parameters of the LDA topic model. There is more detail about these in the second lab notebook. \n",
    "\n",
    "* α – alpha, a Dirichlet prior on the per-document topic distribution\n",
    "* β – beta / eta, a Dirichlet prior on the per-topic word distribution\n",
    "* k – the number of topics in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8398691-abcd-4b55-ad6b-1a5ad1df96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics to return, between 1 and 32767\n",
    "num_topics = 3\n",
    "\n",
    "# You can read more about the following alpha and beta hyperparameters here:\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "# Alpha\n",
    "# Hyperparameter of Dirichlet distribution for document-topic\n",
    "# Controls the density of topics per document\n",
    "# a float\n",
    "doc_topic = 0.1\n",
    "\n",
    "# Beta\n",
    "# Hyperparameter of Dirichlet distribution for topic-word\n",
    "# Note this is 'eta' in tomotopy - it's not a typo!\n",
    "# Controls the density of words per topic\n",
    "# a float\n",
    "topic_word = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021aa32f-5b7e-4633-939c-84058af2b9e3",
   "metadata": {},
   "source": [
    "**Run the cell below to train the model and display the results**\n",
    "\n",
    "(you shouldn't need to change anything in this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d97b-63e2-4011-a51f-80e086333740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://bab2min.github.io/tomotopy/v0.12.2/en/\n",
    "\n",
    "# Intialize the model\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE,\n",
    "                    k=num_topics, \n",
    "                    alpha=doc_topic, \n",
    "                    eta=topic_word\n",
    "                   )\n",
    "\n",
    "num_topic_words = 10 # we'll display 10 words to represent each topic\n",
    " \n",
    "# Add each document to the model\n",
    "for text in doc_clean:\n",
    "    model.add_doc(text)\n",
    "    \n",
    "print(\"Topic Model Training...\\n\")\n",
    "\n",
    "# train the model\n",
    "# we have specified 200 (10*20) iterations of Gibbs sampling total\n",
    "# the loop reports Log Liklihood/word every 20 iterations. This is a measure of model fit to the data (higher is better)\n",
    "for i in range(0, 10):\n",
    "    model.train(iter=20)\n",
    "    print(f'Iteration: {i}\\tLog-likelihood: {model.ll_per_word}')\n",
    "\n",
    "# print topic words\n",
    "for k in range(model.k):\n",
    "    print('\\nTop 10 words of topic #{}\\n'.format(k))\n",
    "    print(model.get_topic_words(k, top_n=num_topic_words)) # here we request 10 words to represent each topic\n",
    "\n",
    "# get info about the model we just trained    \n",
    "print(\"\\nModel Summary\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bf53d-176a-462b-a756-b1db0217b782",
   "metadata": {},
   "source": [
    "### Examine top documents for a given topic (Use Example 2 data for this)\n",
    "The following code to display the top documents is adapated from [***Topic Modeling - With Tomotopy***](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html) from the book *Introduction to Cultural Analytics & Python* by Melanie Walsh (2021).\n",
    "\n",
    "First, load the topic distributions...  \n",
    "_Needs running every time you retrain / update the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca5faf-d7bd-487e-a716-fe76434bfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, model.k):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-legislation",
   "metadata": {},
   "source": [
    "Optional: If you're interested in the code, you can uncomment lines and run the cell below to see what the variables in the cell above contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions # topic proportions in documents as a list\n",
    "\n",
    "#topics # topic words as a string\n",
    "\n",
    "#topic_individual_words # topic words as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-columbia",
   "metadata": {},
   "source": [
    "Next are a couple of functions to format our output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_md(string):\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, topic_distributions, topic_index, n=5):\n",
    "    \n",
    "    sorted_data = sorted([(_distribution[topic_index], _document) \n",
    "                          for _distribution, _document\n",
    "                          in zip(topic_distributions, docs)], reverse=True)\n",
    "    \n",
    "    topic_words = topics[topic_index]\n",
    "    \n",
    "    make_md(f\"### Topic {topic_index}\\n\\n{topic_words}\\n\\n---\")\n",
    "    \n",
    "    for probability, doc in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        bolded = []\n",
    "        for word in doc[1].split():\n",
    "            if word.lower().strip(')(.?!,') in topic_words.split():\n",
    "                bolded.append('**{}**'.format(word))\n",
    "            else:\n",
    "                bolded.append(word)\n",
    "        \n",
    "        make_md(f'  \\n**Topic Probability**: {probability}  \\n**Source**: {doc[0]}  \\n**Document**: {\" \".join(bolded)} \\n\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-acceptance",
   "metadata": {},
   "source": [
    "Run the next cell to list the topics and top n documents in each...\n",
    "\n",
    "The number of words representing the topic and bolded in the text is set by the ```num_topic_words``` variable in the section 'Train a topic model'.  \n",
    "_Needs running every time you retrain / update the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7273b-32ac-4de4-b886-4ff4005ff29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_num in range(model.k):\n",
    "    get_top_docs(docs_and_source, topic_distributions, topic_index=topic_num, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-accounting",
   "metadata": {},
   "source": [
    "#### Things to look for\n",
    "\n",
    "Can you see how increasing the number of topics starts to produce more specific sub-topics? We don't have just \"cats\" and \"dogs\", but topics like \"humans' connection with dogs\", or \"how cats help humans\", or \"the origins of dogs & wolves\". \n",
    "\n",
    "Of course, these are not exact categorisations, but with enough data and well-chosen model parameters, LDA topic modeling can produce useful and interesting topic groupings. \n",
    "\n",
    "When you have a _lot_ of documents, it can be hard to interpret topics and understand whether they are coherent or not. We will see some other approaches to assessing coherence, but in all cases qualitative assessment of the topics will be very important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa840f71",
   "metadata": {},
   "source": [
    "### On the tradeoff between words in topics and topics in documents (use Examples 2 and 3 for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859f630",
   "metadata": {},
   "source": [
    "If you haven't done so already, read footnote 2 in the David Blei article [\"Topic Modeling and Digital Humanities\"](https://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/#topic-modeling-and-digital-humanities-by-david-m-blei-n-2).\n",
    "\n",
    "We will create some ways to begin visualising the intuition Blei describes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23da049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll construct a document-topic matrix\n",
    "# rows are documents, columns are topics\n",
    "# values are frequency of words assigned to this topic in this document\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "doc_topic_df = pd.DataFrame(0, index=np.arange(len(model.docs)), columns=np.arange(model.k))\n",
    "\n",
    "for row, doc in enumerate(model.docs):\n",
    "    doc_topic_counts = Counter(doc.topics)\n",
    "    for k, v in doc_topic_counts.items():\n",
    "        doc_topic_df.iloc[row, k] = v\n",
    "\n",
    "doc_topic_df.columns = ['Topic ' + str(t) for t in range(model.k)]\n",
    "\n",
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the dataframe, then plot a bar chart per document\n",
    "# Don't use this with large numbers of documents!\n",
    "\n",
    "h = len(model.docs)\n",
    "\n",
    "axes = doc_topic_df.T.plot.bar(subplots=True, \n",
    "                                figsize=(h/2,h*2),\n",
    "                                title=['Doc '+ str(num) for num in range(len(model.docs))],\n",
    "                                legend=False,\n",
    "                                sharex=False,\n",
    "                                sharey=True,\n",
    "                                ylabel='Frequency')\n",
    "\n",
    "fig = axes[0].get_figure()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28381c",
   "metadata": {},
   "source": [
    "The code block below produces a heat map showing the density of topic-word distributions for each topic. The y-axis is the topics and the x-axis is the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a2f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_dist_list = []\n",
    "\n",
    "for t in range(model.k):\n",
    "    topic_dist_list.append(model.get_topic_word_dist(t, normalize=True))\n",
    "\n",
    "topic_dist_df = pd.DataFrame(topic_dist_list)\n",
    "vocab = model.vocabs # Get the vocabulary from the model\n",
    "\n",
    "plt.figure(figsize=(29, 4))  # Adjust the width and height of the plot\n",
    "ax = sns.heatmap(topic_dist_df, cmap=\"YlGnBu\", linewidth = 0.5,)\n",
    "plt.title(f'Probability of words in topics - Example {example_number}', fontsize=12)\n",
    "plt.xlabel('Words (Vocabulary)', fontsize=10)\n",
    "plt.ylabel('Topics', fontsize=10)\n",
    "\n",
    "ax.xaxis.set_ticks_position('none') \n",
    "ax.set_xticks([x + 0.5 for x in range(len(vocab))])  # This just sets the positions of the labels to the centre of each cell\n",
    "ax.set_xticklabels(vocab, rotation=90, fontsize=10, ha='center')  # This sets the labels to the actual words and centres them\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee67f5-f887-4606-8771-1a94f079c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can review the top words in our topics and compare them to the chart\n",
    "for k in range(model.k):\n",
    "    print('\\nTop 10 words of topic #{}\\n'.format(k))\n",
    "    print(model.get_topic_words(k, top_n=num_topic_words)) # here we request 10 words to represent each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a0c09",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "<strong>Task 3:</strong> Using the bar chart and heat maps, try some comparisons:\n",
    "   <ul>\n",
    "       <li>Example 2 (fewer documents) vs Example 3 (more documents), keeping the number of topics constant.</li>\n",
    "       <li>Example 2, with 3 topics, then 6 topics.\n",
    "       <li>Example 3, with 3 topics, then 6 topics.\n",
    "       <li>Experiment with changing the α (alpha) and β (beta) values.\n",
    "    </ul>\n",
    "    <p>You can open the resulting images in a new tab or copy and paste them to a document to keep a copy, and make some notes about your findings.</p>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-edgar",
   "metadata": {},
   "source": [
    "### Further exercises\n",
    "\n",
    "- Make your own tiny corpus with fewer than 10 documents. Explore how words are assigned to topics, and how topics are spread across documents. See if you can choose documents that have some overlapping topics for the algorithm to pick up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-token",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Cat  \n",
    "- https://en.wikipedia.org/wiki/Dog  \n",
    "- https://cat-world.com/cats-as-pets/  \n",
    "- https://www.goodhousekeeping.com/uk/news/a560588/10-reasons-why-dogs-are-the-best-pets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166976f5-891b-4ce7-bddb-31a1a4c02948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
